{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPEVKncPjqjSlxHBbuqJYnP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meretja/MasterThesis/blob/main/Master_Thesis_Code_Meret_Vivianne_Jaggi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full archive search - Twitter"
      ],
      "metadata": {
        "id": "HdL6ixKZOO7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import csv\n",
        "import datetime\n",
        "import dateutil.parser\n",
        "import unicodedata\n",
        "import time\n",
        "\n",
        "# set a “TOKEN” variable\n",
        "os.environ['TOKEN'] = 'AAAAAAAAAAAAAAAAAAAAAEkOlgEAAAAAeVHffsF0MpKg8khU6JnOoLpHk9M%3DdyGdF3vEqPCY4nBaiNlouPWXYBqCQAYjTT6PiFysIjiZ9fFYoo'\n",
        "\n",
        "# retrieve the token from the environment.\n",
        "def auth():\n",
        "    return os.getenv('TOKEN')\n",
        "\n",
        "# create headers to access the API\n",
        "def create_headers(bearer_token):\n",
        "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
        "    return headers\n",
        "\n",
        "# build request for endpoints\n",
        "def create_url(keyword, start_date, end_date, max_results = 10):\n",
        "\n",
        "    search_url = \"https://api.twitter.com/2/tweets/search/all\" # Change to the endpoint to collect data from\n",
        "\n",
        "    #change params based on the used endpoint\n",
        "    query_params = {'query': keyword,\n",
        "                    'start_time': start_date,\n",
        "                    'end_time': end_date,\n",
        "                    'max_results': max_results,\n",
        "                    'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
        "                    'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
        "                    'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
        "                    'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
        "                    'next_token': {}}\n",
        "    return (search_url, query_params)\n",
        "\n",
        "# connect to endpoint\n",
        "def connect_to_endpoint(url, headers, params, next_token = None):\n",
        "    params['next_token'] = next_token   #params object received from create_url function\n",
        "    response = requests.request(\"GET\", url, headers = headers, params = params)\n",
        "    print(\"Endpoint Response Code: \" + str(response.status_code))\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(response.status_code, response.text)\n",
        "    return response.json()\n",
        "\n",
        "# Inputs for the request\n",
        "bearer_token = auth()\n",
        "headers = create_headers(bearer_token)\n",
        "keyword = \"#mastercard lang:en\"\n",
        "start_time = \"2023-01-27T00:00:00.000Z\"\n",
        "end_time = \"2023-01-28T00:00:00.000Z\"\n",
        "max_results = 10\n",
        "\n",
        "# Create URL and get response from the API\n",
        "url = create_url(keyword, start_time,end_time, max_results)\n",
        "json_response = connect_to_endpoint(url[0], headers, url[1])\n",
        "\n",
        "# print the response in a readable format\n",
        "print(json.dumps(json_response, indent=4, sort_keys=True))\n",
        "\n",
        "# retrieve the next_token\n",
        "json_response['meta']['result_count']\n",
        "\n",
        "# Save response as a csv-file\n",
        "\n",
        "# Create file\n",
        "csvFile = open(\"data.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
        "csvWriter = csv.writer(csvFile)\n",
        "\n",
        "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
        "csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count', 'tweet'])\n",
        "csvFile.close()\n",
        "\n",
        "def append_to_csv(json_response, fileName):\n",
        "\n",
        "    #A counter variable\n",
        "    counter = 0\n",
        "\n",
        "    #Open OR create the target CSV file\n",
        "    csvFile = open(fileName, \"a\", newline=\"\", encoding='utf-8')\n",
        "    csvWriter = csv.writer(csvFile)\n",
        "\n",
        "    #Loop through each tweet\n",
        "    for tweet in json_response['data']:\n",
        "\n",
        "        # We will create a variable for each since some of the keys might not exist for some tweets\n",
        "        # So we will account for that\n",
        "\n",
        "        # 1. Author ID\n",
        "        author_id = tweet['author_id']\n",
        "\n",
        "        # 2. Time created\n",
        "        created_at = dateutil.parser.parse(tweet['created_at'])\n",
        "\n",
        "        # 3. Geolocation\n",
        "        if ('geo' in tweet):\n",
        "            geo = tweet['geo']['place_id']\n",
        "        else:\n",
        "            geo = \" \"\n",
        "\n",
        "        # 4. Tweet ID\n",
        "        tweet_id = tweet['id']\n",
        "\n",
        "        # 5. Language\n",
        "        lang = tweet['lang']\n",
        "\n",
        "        # 6. Tweet metrics\n",
        "        retweet_count = tweet['public_metrics']['retweet_count']\n",
        "        reply_count = tweet['public_metrics']['reply_count']\n",
        "        like_count = tweet['public_metrics']['like_count']\n",
        "        quote_count = tweet['public_metrics']['quote_count']\n",
        "\n",
        "        # 7. source\n",
        "        #source = tweet['source']\n",
        "\n",
        "        # 8. Tweet text\n",
        "        text = tweet['text']\n",
        "\n",
        "        # Assemble all data in a list\n",
        "        res = [author_id, created_at, geo, tweet_id, lang, like_count, quote_count, reply_count, retweet_count, text]\n",
        "\n",
        "        # Append the result to the CSV file\n",
        "        csvWriter.writerow(res)\n",
        "        counter += 1\n",
        "\n",
        "    # When done, close the CSV file\n",
        "    csvFile.close()\n",
        "\n",
        "    # Print the number of tweets for this iteration\n",
        "    print(\"# of Tweets added from this response: \", counter)\n",
        "\n",
        "append_to_csv(json_response, \"data.csv\")"
      ],
      "metadata": {
        "id": "nYU_Gpx3OOw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loop over different time periods to group together certain number of tweets"
      ],
      "metadata": {
        "id": "qMUXabY8OZSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify start and end date\n",
        "start_date = '2018-05-02'\n",
        "end_date = '2020-08-13'\n",
        "\n",
        "# generate a series of days\n",
        "dates = pd.date_range(start_date, end_date)\n",
        "\n",
        "start_dates = dates[0:-1]\n",
        "start_dates = [d.strftime('%Y-%m-%dT%H:%M:%SZ') for d in start_dates]\n",
        "# start_dates = start_dates.isoformat()\n",
        "# start_dates = start_dates.strftime('%Y-%m-%dT%H:%M:%S.%f%z').tolist()\n",
        "\n",
        "end_dates = dates[1:]\n",
        "end_dates = [d.strftime('%Y-%m-%dT%H:%M:%SZ') for d in end_dates]\n",
        "# end_dates = end_dates.strftime('%Y-%m-%dT%H:%M:%S.%f%z').tolist()\n",
        "\n",
        "\n",
        "#Inputs for tweets\n",
        "bearer_token = auth()\n",
        "headers = create_headers(bearer_token)\n",
        "keyword = \"$abbv lang:en\"\n",
        "\n",
        "max_results = 500\n",
        "\n",
        "#Total number of tweets we collected from the loop\n",
        "total_tweets = 0\n",
        "\n",
        "# Create file\n",
        "csvFile = open(\"abbv1.csv\", \"a\", newline=\"\", encoding='utf-8')\n",
        "csvWriter = csv.writer(csvFile)\n",
        "\n",
        "#Create headers for the data you want to save, in this example, we only want save these columns in our dataset\n",
        "csvWriter.writerow(['author id', 'created_at', 'geo', 'id','lang', 'like_count', 'quote_count', 'reply_count','retweet_count','source','tweet'])\n",
        "csvFile.close()\n",
        "\n",
        "for i in range(0,len(start_dates)):\n",
        "\n",
        "    # Inputs\n",
        "    count = 0 # Counting tweets per time period\n",
        "    max_count = 500 # Max tweets per time period\n",
        "    flag = True\n",
        "    next_token = None\n",
        "\n",
        "    # Check if flag is true\n",
        "    while flag:\n",
        "        # Check if max_count reached\n",
        "        if count >= max_count:\n",
        "            break\n",
        "        print(\"-------------------\")\n",
        "        print(\"Token: \", next_token)\n",
        "        url = create_url(keyword, start_dates[i],end_dates[i], max_results)\n",
        "        json_response = connect_to_endpoint(url[0], headers, url[1], next_token)\n",
        "        result_count = json_response['meta']['result_count']\n",
        "\n",
        "        if 'next_token' in json_response['meta']:\n",
        "            # Save the token to use for next call\n",
        "            next_token = json_response['meta']['next_token']\n",
        "            print(\"Next Token: \", next_token)\n",
        "            if result_count is not None and result_count > 0 and next_token is not None:\n",
        "                print(\"Start Date: \", start_dates[i])\n",
        "                append_to_csv(json_response, \"abbv1.csv\")\n",
        "                count += result_count\n",
        "                total_tweets += result_count\n",
        "                print(\"Total # of Tweets added: \", total_tweets)\n",
        "                print(\"-------------------\")\n",
        "                time.sleep(2)\n",
        "        # If no next token exists\n",
        "        else:\n",
        "            if result_count is not None and result_count > 0:\n",
        "                print(\"-------------------\")\n",
        "                print(\"Start Date: \", start_dates[i])\n",
        "                append_to_csv(json_response, \"abbv1.csv\")\n",
        "                count += result_count\n",
        "                total_tweets += result_count\n",
        "                print(\"Total # of Tweets added: \", total_tweets)\n",
        "                print(\"-------------------\")\n",
        "                time.sleep(2)\n",
        "\n",
        "            #Since this is the final request, turn flag to false to move to the next time period.\n",
        "            flag = False\n",
        "            next_token = None\n",
        "        time.sleep(2)\n",
        "print(\"Total number of results: \", total_tweets)"
      ],
      "metadata": {
        "id": "cg8-OmPZOZLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment extraction from sourced tweets"
      ],
      "metadata": {
        "id": "RMe2d7ftN8Hs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplary code of sentiment extraction using TSLA ticker\n",
        "The same was done for each individual ticker, individually, to assign sentiment to each Tweet that was scraped previously"
      ],
      "metadata": {
        "id": "t7L9OFrDOGVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "\"\"\"Load packages and pre-trained model from Hugging Face\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "\"\"\"Define FinBERT model and sentiment analysis task\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Defining FinBERT Model (yiyanghkust/finbert-tone) to yield the following results with the corresponding confidence level:\n",
        "#LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative\n",
        "\"\"\"\n",
        "\n",
        " # use pre-trained yiyanghkust/finbert-tone model and tokeniser\n",
        "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
        "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
        "\n",
        "# define task to be sentiment analysis and set model and tokeniser, accordingly\n",
        "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
        "\n",
        "\"\"\"Load tweets\"\"\"\n",
        "\n",
        "# Set path to csv-files of tweets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Preparing tweets to feed them to the pre-trained model\n",
        "tsla = pd.read_csv('/content/drive/MyDrive/tweets/TSLA.csv')\n",
        "tsla['tweet'] = 'tsla'\n",
        "tsla = tsla.rename(columns={\"tweet\": \"ticker\", \"source\": \"text\"})\n",
        "tsla = tsla.drop(columns=[\"author id\", \"geo\", \"id\", \"lang\", \"like_count\", \"quote_count\", \"reply_count\", \"retweet_count\"])\n",
        "\n",
        "\"\"\"add extra code to separate the dataframe into two parts --> can only run for 24h and these datasets are too large for that\"\"\"\n",
        "\n",
        "# tsla1 = tsla.iloc[:260000,:]\n",
        "# tsla2 = tsla.iloc[260000:520000,:]\n",
        "tsla3 = tsla.iloc[520000:,:]\n",
        "\n",
        "\"\"\"Apply NLP function to the tweets\"\"\"\n",
        "\n",
        "results_tsla3 = []\n",
        "for tweet in tsla3[\"text\"]:\n",
        "    result = nlp(tweet)\n",
        "    results_tsla3.append(result)\n",
        "\n",
        "\"\"\"Add sentiment and confidence level to DataFrame\"\"\"\n",
        "\n",
        "# Extract the sentiment scores and confidence levels from each dictionary\n",
        "sentiment_tsla3 = [result[0][\"label\"] for result in results_tsla3]\n",
        "confidence_tsla3 = [result[0][\"score\"] for result in results_tsla3]\n",
        "\n",
        "# Add the sentiment scores and confidence levels to the DataFrame\n",
        "tsla3[\"sentiment_score\"] = sentiment_tsla3\n",
        "tsla3[\"confidence_level\"] = confidence_tsla3\n",
        "\n",
        "\"\"\"Convert to csv-file\"\"\"\n",
        "\n",
        "tsla3.to_csv('/content/drive/MyDrive/sentiment_tsla3.csv', index=False)\n",
        "\n",
        "tsla3.head()"
      ],
      "metadata": {
        "id": "zvQjpguxN77m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MAIN ANALYSIS\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HV5J2gKiFART"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following file includes the computation of sentiment measures, z-scores, changes in demand as well as conducting the entire set of analyses from the thesis"
      ],
      "metadata": {
        "id": "5RqdND6nFNKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ],
      "metadata": {
        "id": "o6qo06QqMsD3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed4O1fj4En8n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "from functools import reduce\n",
        "import matplotlib.dates as mdates\n",
        "from pykalman import KalmanFilter\n",
        "from scipy.optimize import minimize\n",
        "from tabulate import tabulate\n",
        "from linearmodels.panel import PanelOLS\n",
        "import statsmodels.api as sm\n",
        "from pystout import pystout\n",
        "from scipy import stats\n",
        "from linearmodels import PanelOLS\n",
        "from linearmodels.panel import compare\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import hmmlearn\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# define colours for the subsequent illustrations\n",
        "colors = sns.color_palette('Set2', n_colors=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Scores"
      ],
      "metadata": {
        "id": "3DYeg1LbMmo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import sentiment data scraped from Twitter and stored in .csv files"
      ],
      "metadata": {
        "id": "lB7jnIFRMfMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/Users/meret/Desktop/Uni/Master_Thesis/Data/sentiment_extraction'\n",
        "filenames = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for filename in filenames:\n",
        "    temp_df = pd.read_csv(filename)\n",
        "    dfs.append(temp_df)\n",
        "\n",
        "# transform list of dataframes into one single dataframe\n",
        "sentiment_data = pd.concat(dfs)\n",
        "\n",
        "# create boolean mask to delete any entries in the \"created_at\" column that are not a date (i.e. not 25 characters long)\n",
        "sentiment_data['created_at'] = sentiment_data['created_at'].fillna(-1)\n",
        "mask = sentiment_data['created_at'].apply(lambda x: len(str(x))) == 25\n",
        "sentiment_data = sentiment_data.loc[mask]\n",
        "\n",
        "sentiment_data['created_at'] = pd.to_datetime(sentiment_data['created_at'])\n",
        "sentiment_data['date'] = sentiment_data['created_at'].dt.date\n",
        "sentiment_data = sentiment_data.drop(columns=['created_at'])"
      ],
      "metadata": {
        "id": "vbhGoZiwMfEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sort confidence_level and inspect number of entries below a certain threshold value"
      ],
      "metadata": {
        "id": "qh9qROUaMa9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check whether there are string values in confidence_level\n",
        "string_values = sentiment_data['confidence_level'].apply(lambda x: isinstance(x, str))\n",
        "\n",
        "# RESULT: over a million entries have a string instead of float value\n",
        "    # --> convert to float value in order to sort them\n",
        "\n",
        "# Convert string values to float\n",
        "sentiment_data['confidence_level'] = pd.to_numeric(sentiment_data['confidence_level'], errors='coerce')\n",
        "\n",
        "# Count entries that have confidence level below certain threshold values\n",
        "below_threshold_fifty = (sentiment_data['confidence_level'] < 0.5).sum()    # sum: 4'871\n",
        "below_threshold_seventy = (sentiment_data['confidence_level'] < 0.7).sum()  # sum: 142'281\n",
        "below_threshold_ninety = (sentiment_data['confidence_level'] < 0.9).sum()   # sum: 356'948"
      ],
      "metadata": {
        "id": "t1uERCEBMbhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frequency of sentiment categories per day"
      ],
      "metadata": {
        "id": "fSPagCjVMQNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative = sentiment_data[sentiment_data['sentiment_score'] == 'Negative'].groupby(['date', 'ticker'])['sentiment_score'].value_counts().sort_index()\n",
        "negative = negative.rename('negative').reset_index()[['date', 'ticker', 'negative']]\n",
        "negative = negative.set_index('date')\n",
        "\n",
        "neutral = sentiment_data[sentiment_data['sentiment_score'] == 'Neutral'].groupby(['date', 'ticker'])['sentiment_score'].value_counts().sort_index()\n",
        "neutral = neutral.rename('neutral').reset_index()[['date', 'ticker', 'neutral']]\n",
        "neutral = neutral.set_index('date')\n",
        "\n",
        "positive = sentiment_data[sentiment_data['sentiment_score'] == 'Positive'].groupby(['date', 'ticker'])['sentiment_score'].value_counts().sort_index()\n",
        "positive = positive.rename('positive').reset_index()[['date', 'ticker', 'positive']]\n",
        "positive = positive.set_index('date')\n",
        "\n",
        "# Combine dataframes\n",
        "sentiment_list = [neutral, negative, positive]\n",
        "sentiment = reduce(lambda left,right: pd.merge(left,right,on=['date','ticker'], how='outer'), sentiment_list)\n",
        "sentiment.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "9xejrs_JMQET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add sentiment-weighted count-based measure"
      ],
      "metadata": {
        "id": "uita7jsUMLIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = sentiment.reset_index()\n",
        "\n",
        "for index, row in sentiment.iterrows():\n",
        "\n",
        "    neut = row['neutral']\n",
        "    neg = row['negative']\n",
        "    pos = row['positive']\n",
        "\n",
        "    # Calculate weighted count-based measure\n",
        "    weighted = (neg*0.5) + (neut*1) + (pos*1.5)\n",
        "    weighted_avg = weighted / (neg + neut + pos)\n",
        "\n",
        "    sentiment.at[index, 'weighted'] = weighted\n",
        "    sentiment.at[index, 'weighted_avg'] = weighted_avg"
      ],
      "metadata": {
        "id": "a5e6X0n0MK-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deal with missing BRK.B entries"
      ],
      "metadata": {
        "id": "6m1U5AJBMEYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = sentiment.sort_values(by=['ticker', 'date'])\n",
        "grouped_data = sentiment.groupby('ticker')\n",
        "grouped_data.size() # shows that BRK.B has a few entries missing\n",
        "\n",
        "brkb_sentiment = sentiment[sentiment['ticker'] == 'brkb'].iloc[:,:]\n",
        "\n",
        "# create DataFrame of 834 dates\n",
        "sentiment = sentiment.set_index('date')\n",
        "date_range = pd.date_range(start=sentiment.index.min(), end=sentiment.index.max(), freq='D')\n",
        "date_df = pd.DataFrame({'date': date_range})\n",
        "\n",
        "# transform both date series to datetime64 to merge\n",
        "date_df['date'] = pd.to_datetime(date_df['date'])\n",
        "brkb_sentiment['date'] = pd.to_datetime(brkb_sentiment['date'])\n",
        "\n",
        "# add entry for each day in the observation period\n",
        "merged_brkb = pd.merge(brkb_sentiment, date_df, on='date', how='outer')\n",
        "merged_brkb['ticker'].fillna('brkb', inplace=True)\n",
        "merged_brkb.fillna(0, inplace=True)\n",
        "merged_brkb['weighted_avg'] = merged_brkb['weighted_avg'].replace(0, method='ffill')\n",
        "\n",
        "# add new BRKB series to original sentiment dataframe\n",
        "sentiment = sentiment.reset_index()\n",
        "sentiment = sentiment.drop(sentiment[sentiment['ticker'] == 'brkb'].index)\n",
        "sentiment['date'] = pd.to_datetime(sentiment['date'])\n",
        "sentiment = pd.concat([sentiment, merged_brkb])\n",
        "\n",
        "sentiment = sentiment.sort_values(by=['ticker', 'date'])\n",
        "sentiment = sentiment.set_index('date')\n",
        "\n",
        "# check that we now have enough entries for each ticker\n",
        "grouped_data = sentiment.groupby('ticker')\n",
        "grouped_data.size()"
      ],
      "metadata": {
        "id": "GhTuBChkMEO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kalman filter"
      ],
      "metadata": {
        "id": "RFfBiLTFMAg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Kalman filter function"
      ],
      "metadata": {
        "id": "7uI91wAtL8lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kalman_log_likelihood(params, sentiment_scores):\n",
        "    # Unpack the parameters\n",
        "    initial_state_mean, initial_state_covariance, observation_covariance, transition_matrices, transition_covariance = params\n",
        "\n",
        "    # Create the Kalman filter\n",
        "    kf = KalmanFilter(initial_state_mean=initial_state_mean,\n",
        "                      initial_state_covariance=initial_state_covariance,\n",
        "                      observation_covariance=observation_covariance,\n",
        "                      transition_matrices=transition_matrices,\n",
        "                      transition_covariance=transition_covariance)\n",
        "\n",
        "    # Calculate the log-likelihood of the observed sentiment scores\n",
        "    _, log_likelihoods = kf.filter(sentiment_scores)\n",
        "\n",
        "    return -np.sum(log_likelihoods)"
      ],
      "metadata": {
        "id": "o-srXZ1GL8e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create filtered scores"
      ],
      "metadata": {
        "id": "sCyaIe7zL2q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ticker, stock_data in grouped_data:\n",
        "\n",
        "    # Extract the sentiment scores for the current stock\n",
        "    sentiment_scores = sentiment['weighted_avg'].values\n",
        "\n",
        "    # Estimate the Kalman filter parameters for the current stock using\n",
        "        # Maximum Likelihood Estimation (minimise negative log-likelihood)\n",
        "\n",
        "    initial_guess = (0.0, np.eye(1), np.eye(1), np.eye(1), np.eye(1))  # Initial guess for the parameters\n",
        "    bounds = ((None, None), (None, None), (None, None), (None, None), (None, None))  # Bounds for the parameters\n",
        "    result = minimize(kalman_log_likelihood, initial_guess, args=(sentiment_scores,), bounds=bounds)\n",
        "    estimated_params = result.x\n",
        "\n",
        "    # Unpack the estimated parameters\n",
        "    estimated_initial_state_mean, estimated_initial_state_covariance, estimated_observation_covariance, \\\n",
        "    estimated_transition_matrices, estimated_transition_covariance = estimated_params\n",
        "\n",
        "    # Apply the estimated parameters to the Kalman filter\n",
        "    kf = KalmanFilter(initial_state_mean=estimated_initial_state_mean,\n",
        "                      initial_state_covariance=estimated_initial_state_covariance,\n",
        "                      observation_covariance=estimated_observation_covariance,\n",
        "                      transition_matrices=estimated_transition_matrices,\n",
        "                      transition_covariance=estimated_transition_covariance)\n",
        "\n",
        "    # Apply the Kalman filter to the sentiment scores for the current stock\n",
        "    filtered_scores, _ = kf.filter(sentiment_scores)\n",
        "\n",
        "    sentiment['filtered_scores'] = filtered_scores"
      ],
      "metadata": {
        "id": "ew0iUg2fL2jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual scores (Appendix)"
      ],
      "metadata": {
        "id": "BKMoNSQ5LvgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = sentiment.reset_index()\n",
        "unique_tickers = sentiment['ticker'].unique()\n",
        "ticker_groups = np.array_split(unique_tickers, 2)\n",
        "\n",
        "for group_idx, ticker_group in enumerate(ticker_groups):\n",
        "    fig, axes = plt.subplots(5, 2, figsize=(12, 15))\n",
        "\n",
        "    # Flatten the axes array for easier indexing\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Loop through each ticker in the current group\n",
        "    for idx, ticker in enumerate(ticker_group):\n",
        "        # Get the data for the current ticker\n",
        "        ticker_data = sentiment[sentiment['ticker'] == ticker]\n",
        "\n",
        "        # Plot the pct_change for the current ticker\n",
        "        ax = axes[idx]\n",
        "        ax.plot(ticker_data['date'], ticker_data['weighted_avg'], 'red', label='Unfiltered Scores', linewidth=0.6)\n",
        "        ax.plot(ticker_data['date'], ticker_data['filtered_scores'], 'aqua', label='Filtered Scores', linewidth=0.6)\n",
        "        ax.set_title(ticker)\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.grid(True)\n",
        "\n",
        "        # Format x-axis tick labels to display only years\n",
        "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "        plt.setp(ax.xaxis.get_majorticklabels())\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.set_title(ax.get_title(), fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "\n",
        "    plt.savefig(f\"scores_{group_idx+1}.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "f9r2gdpgLvZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TSLA example"
      ],
      "metadata": {
        "id": "pGMPfBNnLrIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_sentiment_score = sentiment.groupby('date')['filtered_scores'].mean().reset_index()\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(tsla_sentiment.date, tsla_sentiment.weighted_avg, 'red', label='Unfiltered Scores', linewidth=0.6)\n",
        "plt.plot(tsla_sentiment.date, tsla_sentiment.filtered_scores, 'cyan', label='Filtered Scores', linewidth=0.6)\n",
        "plt.title('Raw vs. Filtered Sentiment Scores - TSLA')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jeBT4MH0LrBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test statistical difference between raw vs. filtered score"
      ],
      "metadata": {
        "id": "vumufc13Li12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ticker in grouped_data:\n",
        "    unfiltered_scores = sentiment['weighted_avg']\n",
        "    filtered_scores = sentiment['filtered_scores']\n",
        "\n",
        "    # Normality test (Shapiro-Wilk test)\n",
        "    _, unfiltered_pvalue = stats.shapiro(unfiltered_scores)\n",
        "    _, filtered_pvalue = stats.shapiro(filtered_scores)\n",
        "\n",
        "    # Perform the appropriate test based on normality results\n",
        "    if unfiltered_pvalue > 0.05 and filtered_pvalue > 0.05:\n",
        "\n",
        "        # Perform paired t-test\n",
        "        _, pvalue = stats.ttest_rel(unfiltered_scores, filtered_scores)\n",
        "        test_name = \"Paired t-test\"\n",
        "\n",
        "    else:\n",
        "        # Perform Wilcoxon signed-rank test\n",
        "        _, pvalue = stats.wilcoxon(unfiltered_scores, filtered_scores)\n",
        "        test_name = \"Wilcoxon signed-rank test\"\n",
        "\n",
        "    print(\"Stock:\", ticker[0])\n",
        "    print(\"Normality test p-values:\")\n",
        "    print(test_name + \" p-value:\", pvalue)\n",
        "    print()"
      ],
      "metadata": {
        "id": "IA6SB5fqLivi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distribution of sentiment scores"
      ],
      "metadata": {
        "id": "qHVrbpLqLe0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual tickers"
      ],
      "metadata": {
        "id": "cnjJM8m_LbeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tickers = sentiment['ticker'].unique()\n",
        "ticker_groups = np.array_split(unique_tickers, 2)\n",
        "\n",
        "for group_idx, ticker_group in enumerate(ticker_groups):\n",
        "\n",
        "    fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, ticker in enumerate(ticker_group):\n",
        "        ticker_data = sentiment[sentiment['ticker'] == ticker]\n",
        "        filtered_scores = ticker_data['filtered_scores']\n",
        "\n",
        "        # Plot the histogram for the current ticker\n",
        "        ax = axes[idx]\n",
        "        ax.hist(filtered_scores, bins=100, density=True, color='darkgreen')\n",
        "        ax.set_title(f\"Distribution of Sentiment Scores -- {ticker}\")\n",
        "        ax.set_xlabel(\"Filtered Scores\")\n",
        "        ax.set_ylabel(\"Density\")\n",
        "\n",
        "    for i in range(len(ticker_group), len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.savefig(f\"distribution_sentiment_ind_{group_idx+1}.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3w3T9D09LbWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average score"
      ],
      "metadata": {
        "id": "Zud0CrFPLYSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(mean_sentiment_score['filtered_scores'], bins=100, color=colors[0], linewidth=0.1)\n",
        "plt.title('Distribution of Average Sentiment Scores')\n",
        "plt.savefig('distribution_sentiment_avg.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H4e4YNb5LYMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualise time series of scores"
      ],
      "metadata": {
        "id": "6xzZXMWSLROb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TSLA positive vs. negative and weighted score\n",
        "\n",
        "tsla_sentiment = sentiment[sentiment['ticker'] == 'tsla'].iloc[1:,:]\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(tsla_sentiment.negative, 'r', label='Negative', linewidth=0.3)\n",
        "#plt.plot(tsla_sentiment.neutral, 'b', label='Neutral', linewidth=0.3)\n",
        "plt.plot(tsla_sentiment.positive, 'g', label='Positive', linewidth=0.3)\n",
        "plt.title('Sentiment TSLA')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(tsla_sentiment.date, tsla_sentiment.filtered_scores, linewidth=0.3)\n",
        "plt.title('Weighted Average of Sentiment - TSLA')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# LLY positive vs. negative and weighted score\n",
        "\n",
        "lly_sentiment = sentiment[sentiment['ticker'] == 'lly'].iloc[1:,:]\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(lly_sentiment.negative, 'r', label='Negative', linewidth=0.3)\n",
        "#plt.plot(lly_sentiment.neutral, 'b', label='Neutral', linewidth=0.3)\n",
        "plt.plot(lly_sentiment.positive, 'g', label='Positive', linewidth=0.3)\n",
        "plt.title('Sentiment LLY')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(lly_sentiment.date, lly_sentiment.filtered_scores, linewidth=0.3)\n",
        "plt.title('Weighted Average of Sentiment - LLY')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Aggregate Data\n",
        "\n",
        "mean_sentiment_data = sentiment.groupby('date')[['neutral', 'positive', 'negative', 'filtered_scores']].mean().reset_index()\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mean_sentiment_data.date, mean_sentiment_data.negative, 'r', label='Negative', linewidth=0.5)\n",
        "#plt.plot(lly_sentiment.neutral, 'b', label='Neutral', linewidth=0.3)\n",
        "plt.plot(mean_sentiment_data.date, mean_sentiment_data.positive, 'g', label='Positive', linewidth=0.5)\n",
        "plt.title('Aggregate Sentiment')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mean_sentiment_data.date, mean_sentiment_data.filtered_scores, linewidth=0.3)\n",
        "plt.title('Weighted Average of Aggregate Sentiment')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mtnvClCvLR8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of sentiment scores"
      ],
      "metadata": {
        "id": "k2BHs7i0LMOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for entire data set\n",
        "summary_table = sentiment.describe()\n",
        "sentiment[['neutral', 'negative', 'positive']].describe()\n",
        "sentiment['filtered_scores'].describe()\n",
        "\n",
        "# for average sentiment scores\n",
        "mean_sentiment_score['filtered_scores'].describe()\n",
        "\n",
        "# for individual tickers\n",
        "summary_stats_list = []\n",
        "\n",
        "for ticker, ticker_data in grouped_data:\n",
        "    filtered_scores = ticker_data['filtered_scores']\n",
        "\n",
        "    summary_stats = filtered_scores.describe().to_frame().T\n",
        "    summary_stats['Ticker'] = ticker\n",
        "    summary_stats_list.append(summary_stats)\n",
        "\n",
        "summary_table_ind = pd.concat(summary_stats_list, ignore_index=True)\n",
        "summary_table_ind = summary_table_ind[['Ticker'] + summary_table.columns[:-1].tolist()]\n",
        "summary_table_ind = summary_table_ind.round(2)\n",
        "\n",
        "# Convert the summary table to LaTeX format\n",
        "latex_table = tabulate(summary_table_ind, tablefmt='latex', headers='keys', showindex=False)\n",
        "print(latex_table)"
      ],
      "metadata": {
        "id": "4yJYNOBSLMFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Z-score"
      ],
      "metadata": {
        "id": "2uSjhcgbKbmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define z-score function"
      ],
      "metadata": {
        "id": "2Mlqq3XRKYDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rollingzscore(x, window):\n",
        "    r = x.rolling(window=window)\n",
        "    m = r.mean().shift(1)\n",
        "    s = r.std(ddof=0).shift(1)\n",
        "    z = (x-m)/s\n",
        "    return z"
      ],
      "metadata": {
        "id": "q11ZrVmuKX7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregate scores"
      ],
      "metadata": {
        "id": "VT8EPWQTKR0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_zScore = rollingzscore(mean_sentiment_score['filtered_scores'], 7)\n",
        "weekly_zScore = weekly_zScore.rename('weekly')\n",
        "monthly_zScore = rollingzscore(mean_sentiment_score['filtered_scores'], 30)\n",
        "monthly_zScore = monthly_zScore.rename('monthly')\n",
        "\n",
        "aggregate_z_scores = pd.concat([weekly_zScore, monthly_zScore], axis=1)\n",
        "aggregate_z_scores['date'] = mean_sentiment_score['date']\n",
        "aggregate_z_scores = aggregate_z_scores.set_index('date').reset_index()\n",
        "aggregate_z_scores = aggregate_z_scores.dropna()\n",
        "\n",
        "# Describe\n",
        "description_overall_zScores = aggregate_z_scores.describe()\n",
        "\n",
        "# Visualise\n",
        "\n",
        "# Distribution\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist([aggregate_z_scores['weekly'], aggregate_z_scores['monthly']], label=['Weekly', 'Monthly'], bins=60, color=[colors[0], colors[1]])\n",
        "plt.title('Distribution of Aggregate Z-Scores')\n",
        "plt.legend(fontsize=10, frameon=True)\n",
        "plt.savefig(\"daily_average_zScores_hist.png\")\n",
        "plt.show()\n",
        "\n",
        "# Time series\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(aggregate_z_scores.date, aggregate_z_scores['weekly'], label='Weekly', color=colors[0], linewidth=0.7)\n",
        "plt.plot(aggregate_z_scores.date, aggregate_z_scores['monthly'], label='Monthly', color=colors[1], linewidth=0.7)\n",
        "plt.title('Time Series of Aggregate Z-Scores')\n",
        "plt.legend(fontsize=10, frameon=True)\n",
        "plt.savefig(\"daily_average_zScores_plot.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kdQPlaaXKRtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Individual tickers"
      ],
      "metadata": {
        "id": "uQlukFY1KNFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute z-scores for each ticker, individually\n",
        "z_scores_ind = pd.DataFrame()\n",
        "sentiment  = sentiment.reset_index()\n",
        "\n",
        "for ticker in sentiment['ticker'].unique():\n",
        "\n",
        "    ticker_data = sentiment[sentiment['ticker'] == ticker].copy()\n",
        "\n",
        "    ticker_data['z_scores_weekly'] = rollingzscore(ticker_data['filtered_scores'], 7)\n",
        "    ticker_data['z_scores_monthly'] = rollingzscore(ticker_data['filtered_scores'], 30)\n",
        "\n",
        "    z_scores_ind = z_scores_ind.append(ticker_data[['date', 'ticker', 'z_scores_weekly', 'z_scores_monthly']], ignore_index=True)\n",
        "\n",
        "z_scores_ind = z_scores_ind.sort_values(by=['ticker', 'date'])\n",
        "\n",
        "# Summarise\n",
        "z_scores_ind_weekly = z_scores_ind[['date', 'ticker', 'z_scores_weekly']].dropna()\n",
        "z_scores_ind_monthly = z_scores_ind[['date', 'ticker', 'z_scores_monthly']].dropna()\n",
        "\n",
        "description_ind_zScores_weekly = z_scores_ind_weekly.groupby('ticker').describe()\n",
        "description_ind_zScores_monthly = z_scores_ind_monthly.groupby('ticker').describe()\n",
        "\n",
        "\n",
        "# Visualise using plot\n",
        "unique_tickers = z_scores_ind['ticker'].unique()\n",
        "ticker_groups = np.array_split(unique_tickers, 2)\n",
        "\n",
        "for group_idx, ticker_group in enumerate(ticker_groups):\n",
        "    fig, axes = plt.subplots(5, 2, figsize=(12, 15))\n",
        "\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, ticker in enumerate(ticker_group):\n",
        "        ticker_data = z_scores_ind[z_scores_ind['ticker'] == ticker]\n",
        "\n",
        "        dates = ticker_data['date']\n",
        "        weekly = ticker_data[['date', 'ticker', 'z_scores_weekly']].set_index('date')\n",
        "        monthly = ticker_data[['date', 'ticker', 'z_scores_monthly']].set_index('date')\n",
        "\n",
        "        ax = axes[idx]\n",
        "        ax.plot(dates, weekly['z_scores_weekly'], color=colors[0], label='Weekly Z-Score', linewidth=0.5, alpha=0.8)\n",
        "        ax.plot(dates, monthly['z_scores_monthly'], color=colors[1], label='Monthly Z-Score', linewidth=0.5, alpha=0.7)\n",
        "        ax.set_title(ticker)\n",
        "        ax.set_ylabel('Z-Scores', fontsize=8)\n",
        "        ax.xaxis.set_tick_params(labelsize=8)\n",
        "        ax.legend(fontsize=8, frameon=True)\n",
        "\n",
        "        ax.grid(True)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "\n",
        "    plt.savefig(f\"z_scores_{group_idx+1}.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3C5LNjT4KM7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robinhood Holdings"
      ],
      "metadata": {
        "id": "U8RYWvUzKHnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data sources from Robintrack"
      ],
      "metadata": {
        "id": "KBfPzmw8J6Gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/Users/meret/Desktop/Uni/Master_Thesis/Data/robinhood/selected_stocks'\n",
        "\n",
        "filenames = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "dfs = []\n",
        "for filename in filenames:\n",
        "    temp_df = pd.read_csv(filename)\n",
        "    temp_df['ticker'] = filename[70:-4]\n",
        "    dfs.append(temp_df)\n",
        "\n",
        "# Put together all Robinhood holdings\n",
        "robinhood_holdings = pd.concat(dfs, ignore_index=True)\n",
        "robinhood_holdings['date'] = pd.to_datetime(robinhood_holdings['timestamp']).dt.date\n",
        "robinhood_holdings = robinhood_holdings.drop(columns=['timestamp'])\n",
        "\n",
        "# compute daily mean of holdings\n",
        "robinhood_holdings = robinhood_holdings.groupby(['ticker', 'date']).mean()\n",
        "\n",
        "# set 'date' as index\n",
        "robinhood_holdings = robinhood_holdings.reset_index()\n",
        "\n",
        "# make ticker lowercase to match other dataframes\n",
        "robinhood_holdings['ticker'] = robinhood_holdings['ticker'].str.lower()\n",
        "\n",
        "# rename brk.b to brkb in robinhood data to match sentiment data\n",
        "robinhood_holdings['ticker'] = robinhood_holdings['ticker'].replace('brk.b', 'brkb')"
      ],
      "metadata": {
        "id": "DdAH0HhAJ59B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deal with missing BRK.B entries"
      ],
      "metadata": {
        "id": "wE_6wDisJ1Vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill missing entries of brkb with zero\n",
        "brkb_holdings = robinhood_holdings[robinhood_holdings['ticker'] == 'brkb'].iloc[:,:]\n",
        "\n",
        "# create DataFrame with only the entire 834 dates\n",
        "date_range = pd.date_range(start=robinhood_holdings['date'].min(), end=robinhood_holdings['date'].max(), freq='D')\n",
        "date_df = pd.DataFrame({'date': date_range})\n",
        "\n",
        "# transform both date series to datetime64 to merge\n",
        "date_df['date'] = pd.to_datetime(date_df['date'])\n",
        "brkb_holdings['date'] = pd.to_datetime(brkb_holdings['date'])\n",
        "\n",
        "# add entry for each day in the observation period\n",
        "merged_brkb = pd.merge(brkb_holdings, date_df, on='date', how='outer')\n",
        "merged_brkb['ticker'].fillna('brkb', inplace=True)\n",
        "merged_brkb.fillna(0, inplace=True)\n",
        "\n",
        "# add new BRKB series to original sentiment dataframe\n",
        "robinhood_holdings = robinhood_holdings.drop(robinhood_holdings[robinhood_holdings['ticker'] == 'brkb'].index)\n",
        "robinhood_holdings = pd.concat([robinhood_holdings, merged_brkb])\n",
        "\n",
        "robinhood_holdings['date'] = pd.to_datetime(robinhood_holdings['date'])\n",
        "robinhood_holdings = robinhood_holdings.sort_values(by=['ticker', 'date'])\n",
        "robinhood_holdings = robinhood_holdings.set_index('date')"
      ],
      "metadata": {
        "id": "l_EC_JliJ1NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create change in holdings"
      ],
      "metadata": {
        "id": "g04v20mVJwEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute change in holdings and save in new dataframe\n",
        "robinhood_holdings['pct_change'] = robinhood_holdings.groupby('ticker')['users_holding'].pct_change()\n",
        "robinhood_holdings = robinhood_holdings.reset_index()\n",
        "holdings_change = robinhood_holdings[['date', 'ticker', 'pct_change']]\n",
        "\n",
        "# count and delete NaN from pct_change computation and replace with 0\n",
        "holdings_change.isna().sum()\n",
        "holdings_change = holdings_change[holdings_change.date != '2018-05-02 00:00:00'] # delete first observation with NaN from taking pct_change\n",
        "holdings_change.isna()\n",
        "holdings_change = holdings_change.fillna(0)\n",
        "\n",
        "# delete observations where 'pct_change' == inf by first sorting values and dropping last row\n",
        "holdings_change = holdings_change.sort_values(by='pct_change')[:-1]\n",
        "holdings_change = holdings_change.sort_values(by=['ticker', 'date'])"
      ],
      "metadata": {
        "id": "HItdYANHJv7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot changes in holdings"
      ],
      "metadata": {
        "id": "ZdnHUd0gJq8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual tickers"
      ],
      "metadata": {
        "id": "EDs9A_0KJmna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tickers = holdings_change['ticker'].unique()\n",
        "ticker_groups = np.array_split(unique_tickers, 2)\n",
        "\n",
        "for group_idx, ticker_group in enumerate(ticker_groups):\n",
        "\n",
        "    fig, axes = plt.subplots(5, 2, figsize=(12, 15))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, ticker in enumerate(ticker_group):\n",
        "        ticker_data = holdings_change[holdings_change['ticker'] == ticker]\n",
        "\n",
        "        ax = axes[idx]\n",
        "        ax.plot(ticker_data['date'], ticker_data['pct_change'], linewidth=0.5)\n",
        "        ax.set_title(ticker)\n",
        "        ax.set_ylabel('Pct Change')\n",
        "        ax.grid(True)\n",
        "\n",
        "        ax.xaxis.set_major_locator(mdates.YearLocator())\n",
        "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
        "        plt.setp(ax.xaxis.get_majorticklabels())\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.set_title(ax.get_title(), fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "\n",
        "    plt.savefig(f\"group_{group_idx+1}_tickers.png\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "cLXZp3-nJmgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examples - TSLA and LLY"
      ],
      "metadata": {
        "id": "XwTsQd1iJgkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TSLA\n",
        "tsla_demand = holdings_change[holdings_change['ticker'] == 'tsla'].iloc[1:,:]\n",
        "plt.style.use('seaborn')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(tsla_demand.date, tsla_demand['pct_change'], color=colors[0], linewidth=0.6)\n",
        "plt.title('Change in Holdings of TSLA Stock')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# LLY\n",
        "lly_demand = holdings_change[holdings_change['ticker'] == 'lly'].iloc[1:,:]\n",
        "plt.style.use('seaborn')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(lly_demand.date, lly_demand['pct_change'], color=colors[0],linewidth=0.6)\n",
        "plt.title('Change in Holdings of LLY Stock')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DdMX5JWCJgYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation Analysis"
      ],
      "metadata": {
        "id": "wcb83bUHJEk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create correlation data"
      ],
      "metadata": {
        "id": "bhYP14DoJJyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_data = holdings_change.merge(sentiment[['date', 'ticker', 'filtered_scores']], on=['date', 'ticker'])\n",
        "# check for NaN --> should be none\n",
        "correlation_data.isna().sum()\n",
        "# group by ticker\n",
        "grouped_corr_data = correlation_data.groupby('ticker')\n",
        "\n",
        "# create new data frame where filtered scores are shifted by one\n",
        "    # aimed to test how 1-day lagged sentiment is correlated with change in demand\n",
        "\n",
        "shifted_data = pd.DataFrame()\n",
        "\n",
        "# Loop through each ticker in the grouped data for one-day lag\n",
        "for ticker, group_df in grouped_corr_data:\n",
        "\n",
        "    group_df['shifted_scores'] = group_df['filtered_scores'].shift(periods=1)\n",
        "    shifted_data = pd.concat([shifted_data, group_df], ignore_index=True)\n",
        "\n",
        "# drop NaNs generated by the shift\n",
        "shifted_data.isna().sum()\n",
        "shifted_data = shifted_data.dropna()"
      ],
      "metadata": {
        "id": "7seqXZH6JJJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lagged correlation"
      ],
      "metadata": {
        "id": "DUoiIKqTJKCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create empty list to store correlation results\n",
        "lagged_correlation_results = []\n",
        "grouped_corr_data = shifted_data.groupby('ticker')\n",
        "\n",
        "window_size = 7\n",
        "\n",
        "for ticker, group in grouped_corr_data:\n",
        "\n",
        "    # normalise data\n",
        "    mean_change = np.mean(group['pct_change'])\n",
        "    norm_change = group['pct_change'] - mean_change\n",
        "    mean_score = np.mean(group['shifted_scores'])\n",
        "    norm_score = group['shifted_scores'] - mean_score\n",
        "\n",
        "    # compute cross correlation between change in holdings and sentiment score\n",
        "    rolling_corr = norm_change.rolling(window=window_size).corr(norm_score)\n",
        "\n",
        "    cross_corr = np.correlate(group['pct_change'], group['shifted_scores'], mode='same')\n",
        "\n",
        "    # norm_cross_corr = np.correlate(norm_change, norm_score)[0]\n",
        "    # norm_cross_corr /= (len(norm_change) * group['pct_change'].std() * group['shifted_scores'].std())\n",
        "\n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['ticker'] = group['ticker']\n",
        "    temp_df['date'] = group['date']\n",
        "    temp_df['corr_coeff'] = cross_corr\n",
        "    temp_df['rolling_corr'] = rolling_corr\n",
        "\n",
        "    lagged_correlation_results.append(temp_df)\n",
        "\n",
        "lagged_correlation_results = pd.concat(lagged_correlation_results, ignore_index=True)\n",
        "lagged_correlation_results = lagged_correlation_results[['date', 'ticker', 'rolling_corr']]\n",
        "\n",
        "# drop NaNs and replace infinity with zeros\n",
        "lagged_correlation_results = lagged_correlation_results.dropna()\n",
        "lagged_correlation_results.replace([np.inf, -np.inf], 0, inplace=True)"
      ],
      "metadata": {
        "id": "KrzVvXOCJJBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot lagged correlation coefficients over time"
      ],
      "metadata": {
        "id": "Ogrocg6rINoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual stocks"
      ],
      "metadata": {
        "id": "IyZxVo9QINiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tickers = lagged_correlation_results['ticker'].unique()\n",
        "ticker_groups = np.array_split(unique_tickers, 2)\n",
        "\n",
        "for group_idx, ticker_group in enumerate(ticker_groups):\n",
        "\n",
        "    fig, axes = plt.subplots(5, 2, figsize=(12, 15))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, ticker in enumerate(ticker_group):\n",
        "        ticker_data = lagged_correlation_results[lagged_correlation_results['ticker'] == ticker]\n",
        "\n",
        "        # Plot the correlation coefficient for the current ticker\n",
        "        ax = axes[idx]\n",
        "        ax.plot(ticker_data['rolling_corr'], linewidth=0.5, color=colors[0])\n",
        "\n",
        "        # Add polynomial trend line of degree 3\n",
        "        coeffs = np.polyfit(ticker_data.index, ticker_data['rolling_corr'], 3)\n",
        "        trend_line = np.polyval(coeffs, ticker_data.index)\n",
        "        ax.plot(ticker_data.index, trend_line, color=colors[1], linewidth=0.8)\n",
        "\n",
        "        ax.set_title(ticker)\n",
        "        ax.set_ylabel('Lagged Correlation Coefficient')\n",
        "        ax.set_xticklabels([])\n",
        "\n",
        "        ax.grid(True)\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.set_title(ax.get_title(), fontsize=10, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "\n",
        "    plt.savefig(f\"rolling_corr{group_idx+1}.png\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "GyH1pYdFINaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of TSLA"
      ],
      "metadata": {
        "id": "zQzuJrLWIIw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lag_corr_tsla = lagged_correlation_results[lagged_correlation_results['ticker'] == 'tsla']\n",
        "max_corr_row_tsla = lag_corr_tsla.loc[lag_corr_tsla['rolling_corr'].idxmax()]\n",
        "max_corr_index_tsla = max_corr_row_tsla['date']\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(lag_corr_tsla['date'], lag_corr_tsla['rolling_corr'], linewidth=0.4)\n",
        "# plt.plot(max_corr_index_tsla, max_corr_row_tsla['rolling_corr'], marker='o', color='red')\n",
        "plt.ylabel('Lagged Correlation Coefficient')\n",
        "plt.title('Lagged Correlation of Sentiment and Changes in User Holdings - TSLA')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SbH4gu6vIIo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average correlation coefficient"
      ],
      "metadata": {
        "id": "T16EtJXlIEYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_correlations = pd.DataFrame(lagged_correlation_results.groupby('ticker')['rolling_corr'].mean())\n",
        "\n",
        "plt.hist(lagged_correlation_results['rolling_corr'], bins=100, color='green', linewidth=0.1)\n",
        "plt.title('Distribution of Correlation Coefficients Across Stocks')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NhAOkNl1IENb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregate Data"
      ],
      "metadata": {
        "id": "icisKZuDH7vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create aggregated data frame\n",
        "correlation_data_aggr = pd.DataFrame()\n",
        "\n",
        "# compute aggregated filtered scores and pct_change\n",
        "correlation_data_aggr['aggr_pct_change'] = shifted_data.groupby('date')['pct_change'].mean()\n",
        "correlation_data_aggr['aggr_scores'] = shifted_data.groupby('date')['shifted_scores'].mean()\n",
        "\n",
        "# compute coefficients\n",
        "# cross_corr_aggr = np.correlate(correlation_data_aggr['aggr_pct_change'], correlation_data_aggr['aggr_scores'], mode='same')\n",
        "\n",
        "mean_change_aggr = np.mean(correlation_data_aggr['aggr_pct_change'])\n",
        "norm_change_aggr = correlation_data_aggr['aggr_pct_change'] - mean_change_aggr\n",
        "mean_score_aggr = np.mean(correlation_data_aggr['aggr_scores'])\n",
        "norm_score_aggr = correlation_data_aggr['aggr_scores'] - mean_score_aggr\n",
        "\n",
        "rolling_corr_aggr = norm_change_aggr.rolling(window=window_size).corr(norm_score_aggr)\n",
        "\n",
        "# append to data frame\n",
        "correlation_data_aggr['corr_coeff'] = rolling_corr_aggr\n",
        "\n",
        "# reset index\n",
        "correlation_data_aggr = correlation_data_aggr.reset_index()\n",
        "\n",
        "# define max coefficient\n",
        "max_corr_row_aggr = correlation_data_aggr.loc[correlation_data_aggr['corr_coeff'].idxmax()]\n",
        "max_corr_index_aggr = max_corr_row_aggr['date']"
      ],
      "metadata": {
        "id": "ZslJHjY_H7o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualise with trend line"
      ],
      "metadata": {
        "id": "9OZtnFLiHuBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dates = correlation_data_aggr['date']\n",
        "y = correlation_data_aggr['corr_coeff']\n",
        "\n",
        "# Fit a polynomial trend line\n",
        "degree = 3\n",
        "coeffs = np.polyfit(np.arange(len(dates)), y, degree)\n",
        "trend_line = np.polyval(coeffs, np.arange(len(dates)))\n",
        "\n",
        "# Visualize\n",
        "plt.style.use('seaborn')\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(dates, y, linewidth=0.8, color=colors[0])\n",
        "plt.plot(dates, trend_line, color=colors[1], label='Trend Line', linewidth=0.8)\n",
        "plt.title('Lagged Correlation Coefficient Across Stocks')\n",
        "# plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# descriptive summary\n",
        "correlation_data_aggr['corr_coeff'].describe()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(correlation_data_aggr['corr_coeff'], bins=150, color=colors[0], edgecolor='steelblue', linewidth=0.1)\n",
        "plt.title('Distribution of Correlation Coefficients Across Stocks')\n",
        "plt.show()\n",
        "\n",
        "correlation_data_aggr = correlation_data_aggr.dropna()"
      ],
      "metadata": {
        "id": "fRgH5IQ0Ht66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute statistical significance of correlation coefficients"
      ],
      "metadata": {
        "id": "PVavPIHRHoPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_1samp\n",
        "\n",
        "def test_correlation_significance(correlations, significance_level=0.05):\n",
        "    t_stat, p_value = ttest_1samp(correlations, popmean=0)\n",
        "    if p_value < significance_level:\n",
        "        return True  # Correlation is statistically significant\n",
        "    else:\n",
        "        return False  # Correlation is not statistically significant"
      ],
      "metadata": {
        "id": "uEWd1RzPHoGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual stocks"
      ],
      "metadata": {
        "id": "h3RjU2roHiln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_data = lagged_correlation_results.groupby('ticker')\n",
        "\n",
        "# prepare count for significant and non-significant tickers\n",
        "significant = 0\n",
        "not_significant = 0\n",
        "\n",
        "for ticker, group in grouped_data:\n",
        "\n",
        "    correlations = group['rolling_corr']\n",
        "    ticker = ticker\n",
        "\n",
        "    is_significant = test_correlation_significance(correlations)\n",
        "    if is_significant:\n",
        "        print(\"The correlation for\", ticker, \"is statistically significant.\")\n",
        "        significant += 1\n",
        "    else:\n",
        "        print(\"The correlation for\", ticker, \"is not statistically significant.\")\n",
        "        not_significant += 1\n",
        "\n",
        "print(\"Number of significant correlations:\", significant)\n",
        "print(\"Number of non-significant correlations:\", not_significant)"
      ],
      "metadata": {
        "id": "ZdiZizz2Hiya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregated data"
      ],
      "metadata": {
        "id": "mpOIcwATHcau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlations_aggr = correlation_data_aggr['corr_coeff']\n",
        "\n",
        "# Test significance\n",
        "is_significant = test_correlation_significance(correlations_aggr)\n",
        "if is_significant:\n",
        "    print(\"The correlation is statistically significant.\")\n",
        "else:\n",
        "    print(\"The correlation is not statistically significant.\")"
      ],
      "metadata": {
        "id": "nlBrYeiwHcSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Panel Regression"
      ],
      "metadata": {
        "id": "56dnElvQG826"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import price and volatility of stocks\n",
        "\n",
        "path = '/Users/meret/Desktop/Uni/Master_Thesis/Data/prices'\n",
        "\n",
        "filenames = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "dfs = []\n",
        "for filename in filenames:\n",
        "    temp_df = pd.read_csv(filename)\n",
        "    temp_df['Ticker'] = filename[51:-4]\n",
        "    dfs.append(temp_df)\n",
        "\n",
        "stock_data = pd.concat(dfs, ignore_index=True)\n",
        "stock_data = stock_data[['Date', 'Ticker', 'Adj Close', 'Volume']]\n",
        "\n",
        "# add percentage change in closing prices and volume\n",
        "stock_data['returns'] = stock_data.groupby('Ticker')['Adj Close'].pct_change()\n",
        "stock_data['vol_pct_change'] = stock_data.groupby('Ticker')['Volume'].pct_change()\n",
        "stock_data.dropna(inplace=True)\n",
        "\n",
        "# standard deviation of percentage change\n",
        "tickers = sorted(pd.unique(stock_data['Ticker']))\n",
        "\n",
        "vola = []\n",
        "\n",
        "for ticker in tickers:\n",
        "    std = stock_data[stock_data['Ticker'] == ticker]['returns'].rolling(window=30).std()\n",
        "    vola.append(std)\n",
        "\n",
        "stock_data['price_vola'] = pd.concat(vola)\n",
        "\n",
        "# change some formatting for later\n",
        "stock_data = stock_data.rename(columns={'Date': 'date', 'Ticker': 'ticker', 'Adj Close': 'price', 'Volume': 'volume'})\n",
        "stock_data['ticker'] = stock_data['ticker'].str.lower()\n",
        "stock_data['date'] = pd.to_datetime(stock_data['date'])\n",
        "\n",
        "# Check names of tickers and change them to match the rest of the data\n",
        "stock_data['ticker'].unique()\n",
        "stock_data['ticker'] = stock_data['ticker'].replace({'brk-b': 'brkb', 'meta': 'fb'})\n",
        "\n",
        "# Merge the sentiment, holdings, and stock data on date and ticker\n",
        "\n",
        "regression_data = pd.merge(correlation_data, stock_data, on=['date', 'ticker'])\n",
        "regression_data = regression_data[['date', 'ticker', 'vol_pct_change', 'returns', 'filtered_scores', 'pct_change']]"
      ],
      "metadata": {
        "id": "vqRRXYfnG8_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Same-day data"
      ],
      "metadata": {
        "id": "2uh1qtLfG4tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_index = regression_data.groupby(\"ticker\").date.transform(lambda c: np.arange(len(c)))\n",
        "regression_data[\"numeric_index\"] = numeric_index\n",
        "regression_data = regression_data.set_index([\"ticker\",\"numeric_index\"])\n",
        "\n",
        "# Hybrid model including both time and entity effects\n",
        "fte_model_sameday = PanelOLS.from_formula('pct_change ~ 1 + filtered_scores + returns + vol_pct_change + EntityEffects + TimeEffects', data=regression_data)\n",
        "fte_results_sameday = fte_model_sameday.fit(cov_type='clustered', cluster_entity=True)\n",
        "\n",
        "# print output of FTE model for appendix\n",
        "fte_summary_sameday = fte_results_sameday.summary\n",
        "print(fte_summary_sameday.as_latex())"
      ],
      "metadata": {
        "id": "tlTtxvU6G4hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lagged data"
      ],
      "metadata": {
        "id": "WqyfCwxaGvkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'ticker'\n",
        "regression_data = regression_data.reset_index()\n",
        "regression_data = regression_data.drop(columns={'numeric_index'})\n",
        "grouped_panel_data = regression_data.groupby('ticker')\n",
        "\n",
        "# create new data frame where volume, returns, and scores are shifted by one\n",
        "    # aimed to assess how 1-day lagged variables cause change in demand\n",
        "\n",
        "shifted_panel_data = pd.DataFrame()\n",
        "\n",
        "# Loop through each ticker in the grouped data for one-day lag\n",
        "for ticker, group_df in grouped_panel_data:\n",
        "\n",
        "    group_df['shifted_scores'] = group_df['filtered_scores'].shift(periods=1)\n",
        "    group_df['shifted_returns'] = group_df['returns'].shift(periods=1)\n",
        "    group_df['shifted_volume'] = group_df['vol_pct_change'].shift(periods=1)\n",
        "    shifted_panel_data = pd.concat([shifted_panel_data, group_df], ignore_index=True)\n",
        "\n",
        "# drop NaNs generated by the shift\n",
        "shifted_panel_data.isna().sum()\n",
        "shifted_panel_data = shifted_panel_data.dropna()\n",
        "shifted_panel_data = shifted_panel_data[['date', 'ticker', 'pct_change',\n",
        "                                         'shifted_scores', 'shifted_returns', 'shifted_volume']]\n",
        "\n",
        "grouped_panel_data = shifted_panel_data.groupby('ticker')\n"
      ],
      "metadata": {
        "id": "MtWpwxYJGvcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Panel Regression with lagged variables - Testing different models"
      ],
      "metadata": {
        "id": "shqAELWRGrCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_index = shifted_panel_data.groupby(\"ticker\").date.transform(lambda c: np.arange(len(c)))\n",
        "shifted_panel_data[\"numeric_index\"] = numeric_index\n",
        "shifted_panel_data = shifted_panel_data.set_index([\"ticker\",\"numeric_index\"])\n",
        "\n",
        "# Fit the fixed effects (FE) model\n",
        "fe_model = PanelOLS.from_formula('pct_change ~ 1 + shifted_scores + shifted_returns + shifted_volume + EntityEffects', data=shifted_panel_data)\n",
        "fe_results = fe_model.fit(cov_type='clustered', cluster_entity=True)\n",
        "\n",
        "# Fit the time effects (TE) model\n",
        "te_model = PanelOLS.from_formula('pct_change ~ 1 + shifted_scores + shifted_returns + shifted_volume + TimeEffects', data=shifted_panel_data)\n",
        "te_results = te_model.fit(cov_type='clustered', cluster_entity=True)\n",
        "\n",
        "# Hybrid model including both time and entity effects\n",
        "fte_model = PanelOLS.from_formula('pct_change ~ 1 + shifted_scores + shifted_returns + shifted_volume + EntityEffects + TimeEffects', data=shifted_panel_data)\n",
        "fte_results = fte_model.fit(cov_type='clustered', cluster_entity=True)\n",
        "\n",
        "# Use test to compare the models and assess their appropriateness\n",
        "model_comparison = compare({'FE': fe_results, 'TE': te_results, 'FTE': fte_results})\n",
        "print(model_comparison)\n",
        "\n",
        "comparison_summary = model_comparison.summary\n",
        "print(comparison_summary.as_latex())\n",
        "\n",
        "# print output of FTE model for main text\n",
        "fte_summary = fte_results.summary\n",
        "print(fte_summary.as_latex())"
      ],
      "metadata": {
        "id": "kTNqjev5Gq5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Granger Causality Test"
      ],
      "metadata": {
        "id": "qz_N5LCpGkX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "\n",
        "significance_level = 0.05\n",
        "\n",
        "# First, check whether data is stationary\n",
        "\n",
        "# pct_change\n",
        "X = regression_data_aggr['aggr_pct_change'].values\n",
        "result = adfuller(X)\n",
        "if result[1] <= significance_level:\n",
        "    print(\"Reject H0, data does not have a unit root and is stationary.\")\n",
        "elif result[1] > significance_level:\n",
        "    print(\"Fail to reject H0, data is not stationary and needs to be transformed.\")\n",
        "\n",
        "# sentiment scores\n",
        "X = regression_data_aggr['aggr_scores'].values\n",
        "result = adfuller(X)\n",
        "if result[1] <= significance_level:\n",
        "    print(\"Reject H0, data does not have a unit root and is stationary.\")\n",
        "elif result[1] > significance_level:\n",
        "    print(\"Fail to reject H0, data is not stationary and needs to be transformed.\")"
      ],
      "metadata": {
        "id": "AXvvkBsiGjbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregated Version"
      ],
      "metadata": {
        "id": "b8Of65-3Gelx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the Granger causality test\n",
        "granger_result = grangercausalitytests(regression_data_aggr, maxlag=1)\n",
        "\n",
        "# Extract the p-values from the test results\n",
        "p_value = granger_result[1][0]['ssr_ftest'][1]\n",
        "\n",
        "# Print the p-values\n",
        "if p_value < significance_level:\n",
        "    print(f'Lag {i}: p-value = {p_value}')\n",
        "\n",
        "# Visualise\n",
        "\n",
        "x = regression_data_aggr.index\n",
        "y1 = regression_data_aggr['aggr_pct_change']\n",
        "y2 = regression_data_aggr['aggr_scores']\n",
        "\n",
        "fig, ax1 = plt.subplots(1,1,figsize=(16,9), dpi= 80)\n",
        "ax1.plot(x, y1, color=colors[0], label='Holdings Change', linewidth=1.5)\n",
        "\n",
        "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "ax2.plot(x, y2, color=colors[1], label='Sentiment Score', linewidth=1.5)\n",
        "\n",
        "ax1.tick_params(axis='x', rotation=0, labelsize=12)\n",
        "ax1.set_ylabel('Holdings Change', color=colors[0], fontsize=12)\n",
        "ax1.tick_params(axis='y', rotation=0, labelcolor=colors[0])\n",
        "ax1.grid(alpha=.4)\n",
        "\n",
        "ax2.set_ylabel(\"Sentiment Scores\", color=colors[1], fontsize=12)\n",
        "ax2.tick_params(axis='y', labelcolor=colors[1])\n",
        "ax2.set_title(\"Time Series of Aggregate Holdings Change and Sentiment Scores\", fontsize=18)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OrgRuUkcGecQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Individual stocks"
      ],
      "metadata": {
        "id": "r_q49E14GZtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tickers = regression_data['ticker'].unique()\n",
        "\n",
        "for ticker in unique_tickers:\n",
        "\n",
        "    ticker_data = regression_data[regression_data['ticker'] == ticker]\n",
        "\n",
        "    # Extract the relevant variables\n",
        "    scores = ticker_data['filtered_scores']\n",
        "    pct_change = ticker_data['pct_change']\n",
        "\n",
        "    # Perform the Granger causality test\n",
        "    granger_result = grangercausalitytests(np.column_stack((scores, pct_change)), maxlag=1, verbose=False)\n",
        "\n",
        "    # Extract the p-value from the test result\n",
        "    p_value = granger_result[1][0]['ssr_ftest'][1]\n",
        "\n",
        "    # Determine whether to accept or reject the null hypothesis based on the p-value\n",
        "    if p_value < significance_level:\n",
        "        result = \"Reject (There is Granger causality)\"\n",
        "    else:\n",
        "        result = \"Accept (There is no Granger causality)\"\n",
        "\n",
        "    # Print the result for the current ticker\n",
        "    print(f\"Granger Causality Test for Ticker: {ticker}\")\n",
        "    print(f\"Result: {result}\")\n",
        "    print(\"------------------------------------------\")"
      ],
      "metadata": {
        "id": "d9sRnC4SGZjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regime Switches Using Hidden Markov Model"
      ],
      "metadata": {
        "id": "cUNrUEw9GPtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hmmlearn.hmm import GaussianHMM\n",
        "from hmmlearn import hmm"
      ],
      "metadata": {
        "id": "_uTMSHxlGPcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Individual Stocks"
      ],
      "metadata": {
        "id": "3ForzO2kI6fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment"
      ],
      "metadata": {
        "id": "N_P_AuTQGLPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regimes_sentiment = []\n",
        "grouped_sentiment_data = sentiment.groupby('ticker')\n",
        "\n",
        "unique_tickers = sentiment['ticker'].unique()\n",
        "unique_tickers = np.sort(unique_tickers)\n",
        "\n",
        "ticker_groups = np.array_split(unique_tickers, 2)\n",
        "\n",
        "for group_idx, ticker_group in enumerate(ticker_groups):\n",
        "    fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, ticker in enumerate(ticker_group):\n",
        "        group = grouped_sentiment_data.get_group(ticker)\n",
        "        X = group['filtered_scores'].values.reshape(-1, 1)\n",
        "\n",
        "        # Build the HMM model, fit to the data and predict hidden states\n",
        "        model = hmm.GaussianHMM(n_components=2, covariance_type=\"diag\", n_iter=50, random_state=42)\n",
        "        model.fit(X)\n",
        "        Z = model.predict(X)\n",
        "        states = pd.unique(Z)\n",
        "\n",
        "        temp_df = pd.DataFrame()\n",
        "        temp_df['ticker'] = group['ticker']\n",
        "        temp_df['date'] = group['date']\n",
        "        temp_df['regimes'] = Z\n",
        "        regimes_sentiment.append(temp_df)\n",
        "\n",
        "        ax = axes[idx]\n",
        "        ax.set_title(f\"Regimes of Filtered Sentiment Scores for {ticker}\")\n",
        "\n",
        "        for i in states:\n",
        "            want = (Z == i)\n",
        "            x = group[\"date\"].iloc[want]\n",
        "            y = group[\"filtered_scores\"].iloc[want]\n",
        "            ax.plot(x, y, '.', markersize=8, color=colors[i])\n",
        "\n",
        "        ax.legend(states, fontsize=10, loc='upper left', frameon=True)\n",
        "        ax.grid(True)\n",
        "        ax.set_ylabel(\"Reshaped Filtered Sentiment Scores\", fontsize=10)\n",
        "        ax.xaxis.set_tick_params(labelsize=10)\n",
        "\n",
        "    for i in range(len(ticker_group), len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.savefig(f\"regimes_sentiment_{group_idx+1}.png\")\n",
        "    plt.show()\n",
        "\n",
        "regimes_sentiment = pd.concat(regimes_sentiment, ignore_index=True)"
      ],
      "metadata": {
        "id": "xFtwRCEtGLHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Holdings Change"
      ],
      "metadata": {
        "id": "OE4hQI0ZGHEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regimes_holdings = []\n",
        "grouped_holdings_data = holdings_change.groupby('ticker')\n",
        "\n",
        "unique_tickers = holdings_change['ticker'].unique()\n",
        "unique_tickers = np.sort(unique_tickers)\n",
        "\n",
        "ticker_groups = np.array_split(unique_tickers, 2)\n",
        "\n",
        "for group_idx, ticker_group in enumerate(ticker_groups):\n",
        "    fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, ticker in enumerate(ticker_group):\n",
        "        group = grouped_holdings_data.get_group(ticker)\n",
        "        X = group['pct_change'].values.reshape(-1, 1)\n",
        "\n",
        "        # Build the HMM model, fit to the data and predict hidden states\n",
        "        model = hmm.GaussianHMM(n_components=2, covariance_type=\"diag\", n_iter=50, random_state=42)\n",
        "        model.fit(X)\n",
        "        Z = model.predict(X)\n",
        "        states = pd.unique(Z)\n",
        "\n",
        "        temp_df = pd.DataFrame()\n",
        "        temp_df['ticker'] = group['ticker']\n",
        "        temp_df['date'] = group['date']\n",
        "        temp_df['regimes'] = Z\n",
        "\n",
        "        regimes_holdings.append(temp_df)\n",
        "\n",
        "        ax = axes[idx]\n",
        "        ax.set_title(f\"Regimes of Changes in Holdings for {ticker}\")\n",
        "\n",
        "        for i in states:\n",
        "            want = (Z == i)\n",
        "            x = group[\"date\"].iloc[want]\n",
        "            y = group[\"pct_change\"].iloc[want]\n",
        "            ax.plot(x, y, '.', markersize=8, color=colors[i])\n",
        "\n",
        "        ax.legend(states, fontsize=10, loc='upper left', frameon=True)\n",
        "        ax.grid(True)\n",
        "        ax.set_ylabel(\"Reshaped Changes in Holdings\", fontsize=10)\n",
        "        ax.xaxis.set_tick_params(labelsize=10)\n",
        "\n",
        "    for i in range(len(ticker_group), len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.savefig(f\"regimes_holdings_{group_idx+1}.png\")\n",
        "    plt.show()\n",
        "\n",
        "regimes_holdings = pd.concat(regimes_holdings, ignore_index=True)"
      ],
      "metadata": {
        "id": "9RfkConzGG6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lagged Correlation Coefficients"
      ],
      "metadata": {
        "id": "aJ-_ydPHF_pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regimes_correlation = []\n",
        "grouped_correlation_data = lagged_correlation_results.groupby('ticker')\n",
        "\n",
        "unique_tickers = lagged_correlation_results['ticker'].unique()\n",
        "unique_tickers = np.sort(unique_tickers)\n",
        "\n",
        "ticker_groups = np.array_split(unique_tickers, 2)\n",
        "\n",
        "for group_idx, ticker_group in enumerate(ticker_groups):\n",
        "    fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, ticker in enumerate(ticker_group):\n",
        "        group = grouped_correlation_data.get_group(ticker)\n",
        "        X = group['rolling_corr'].values.reshape(-1, 1)\n",
        "\n",
        "        # Build the HMM model, fit to the data and predict hidden states\n",
        "        model = hmm.GaussianHMM(n_components=2, covariance_type=\"diag\", n_iter=50, random_state=42)\n",
        "        model.fit(X)\n",
        "        Z = model.predict(X)\n",
        "        states = pd.unique(Z)\n",
        "\n",
        "        temp_df = pd.DataFrame()\n",
        "        temp_df['ticker'] = group['ticker']\n",
        "        temp_df['date'] = group['date']\n",
        "        temp_df['regimes'] = Z\n",
        "\n",
        "        regimes_correlation.append(temp_df)\n",
        "\n",
        "        ax = axes[idx]\n",
        "        ax.set_title(f\"Regimes of Lagged Correlation Coefficients for {ticker}\")\n",
        "\n",
        "        for i in states:\n",
        "            want = (Z == i)\n",
        "            x = group[\"date\"].iloc[want]\n",
        "            y = group[\"rolling_corr\"].iloc[want]\n",
        "            ax.plot(x, y, '.', markersize=8, color=colors[i])\n",
        "\n",
        "        ax.legend(states, fontsize=10, loc='upper left', frameon=True)\n",
        "        ax.grid(True)\n",
        "        ax.set_ylabel(\"Reshaped Correlation Coefficients\", fontsize=10)\n",
        "        ax.xaxis.set_tick_params(labelsize=10)\n",
        "\n",
        "    for i in range(len(ticker_group), len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.savefig(f\"regimes_correlation_{group_idx+1}.png\")\n",
        "    plt.show()\n",
        "\n",
        "regimes_correlation = pd.concat(regimes_correlation, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "V2qe5C23F_gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regimes for Aggregated Data"
      ],
      "metadata": {
        "id": "oziLKUvPFzsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors = sns.color_palette('Set2', n_colors=3)\n",
        "dates = sentiment['date']\n",
        "aggr_regimes = pd.DataFrame(dates)"
      ],
      "metadata": {
        "id": "39416stHFziV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment"
      ],
      "metadata": {
        "id": "fcd4k8RpFwXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_avg_sentiment = sentiment.groupby('date')['filtered_scores'].mean().reset_index()\n",
        "\n",
        "X = daily_avg_sentiment['filtered_scores'].values.reshape(-1, 1)\n",
        "model = hmm.GaussianHMM(n_components=2, covariance_type=\"diag\", n_iter=50, random_state=42)\n",
        "HMMfit_sentiment = model.fit(X)\n",
        "proba_sentiment = HMMfit_sentiment.predict_proba(X)\n",
        "Z = model.predict(X)\n",
        "states = pd.unique(Z)\n",
        "\n",
        "aggr_regime_sentiment = pd.Series(Z, name='sentiment')\n",
        "aggr_regime_sentiment = pd.concat([aggr_regime_sentiment, daily_avg_sentiment['date']], axis=1)\n",
        "aggr_regimes = aggr_regimes.merge(aggr_regime_sentiment, on='date')\n",
        "\n",
        "daily_avg_sentiment['regimes'] = aggr_regime_sentiment['sentiment']\n",
        "\n",
        "# Visualise\n",
        "\n",
        "ax = axes[idx]\n",
        "\n",
        "for i in states:\n",
        "    want = (Z == i)\n",
        "    x = daily_avg_sentiment[\"date\"].iloc[want]\n",
        "    y = daily_avg_sentiment[\"filtered_scores\"].iloc[want]\n",
        "    plt.plot(x, y, '.', markersize=8, color=colors[i])\n",
        "\n",
        "plt.legend(states, fontsize=10, loc='upper left', frameon=True)\n",
        "plt.ylabel(\"Reshaped Filtered Sentiment Scores\", fontsize=10)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.title(\"Regimes of Average Daily Filtered Sentiment Scores\")\n",
        "plt.savefig('regimes_avg_sentiment.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l4FpC4vEFwOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Holdings Changes"
      ],
      "metadata": {
        "id": "REYRE_zbFraP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_avg_holdings = holdings_change.groupby('date')['pct_change'].mean().reset_index()\n",
        "\n",
        "X = daily_avg_holdings['pct_change'].values.reshape(-1, 1)\n",
        "model = hmm.GaussianHMM(n_components=2, covariance_type=\"diag\", n_iter=50, random_state=42)\n",
        "HMMfit_holdings = model.fit(X)\n",
        "proba_holdings = HMMfit_holdings.predict_proba(X)\n",
        "Z = model.predict(X)\n",
        "states = pd.unique(Z)\n",
        "\n",
        "aggr_regime_holdings = pd.Series(Z, name='holdings')\n",
        "aggr_regime_holdings = pd.concat([aggr_regime_holdings, daily_avg_holdings['date']], axis=1)\n",
        "aggr_regimes = aggr_regimes.merge(aggr_regime_holdings, on='date')\n",
        "\n",
        "daily_avg_holdings['regimes'] = aggr_regime_holdings['holdings']\n",
        "\n",
        "# Visualise\n",
        "ax = axes[idx]\n",
        "\n",
        "for i in states:\n",
        "    want = (Z == i)\n",
        "    x = daily_avg_holdings[\"date\"].iloc[want]\n",
        "    y = daily_avg_holdings[\"pct_change\"].iloc[want]\n",
        "    plt.plot(x, y, '.', markersize=8, color=colors[i])\n",
        "\n",
        "plt.legend(states, fontsize=10, loc='upper left', frameon=True)\n",
        "plt.ylabel(\"Reshaped Holdings Change\", fontsize=10)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.title(\"Regimes of Average Daily Changes in Holdings\")\n",
        "plt.savefig('regimes_avg_holdings.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aQRsVVahFrRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lagged Correlation Coefficients"
      ],
      "metadata": {
        "id": "llNy1_EqFkUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_avg_correlation = correlation_data_aggr[['date', 'corr_coeff']]\n",
        "\n",
        "X = daily_avg_correlation['corr_coeff'].values.reshape(-1, 1)\n",
        "model = hmm.GaussianHMM(n_components=2, covariance_type=\"diag\", n_iter=50, random_state=42)\n",
        "HMMfit_correlation = model.fit(X)\n",
        "proba_correlation = HMMfit_correlation.predict_proba(X)\n",
        "Z = model.predict(X)\n",
        "states = pd.unique(Z)\n",
        "\n",
        "aggr_regime_correlation = pd.Series(Z, name='correlation')\n",
        "aggr_regime_correlation = pd.concat([aggr_regime_correlation, correlation_data_aggr['date']], axis=1)\n",
        "aggr_regimes = aggr_regimes.merge(aggr_regime_correlation, on='date')\n",
        "\n",
        "daily_avg_correlation['regimes'] = aggr_regime_correlation['correlation']\n",
        "\n",
        "# Visualise\n",
        "ax = axes[idx]\n",
        "\n",
        "for i in states:\n",
        "    want = (Z == i)\n",
        "    x = daily_avg_correlation[\"date\"].iloc[want]\n",
        "    y = daily_avg_correlation[\"corr_coeff\"].iloc[want]\n",
        "    plt.plot(x, y, '.', markersize=8, color=colors[i])\n",
        "\n",
        "plt.legend(states, fontsize=10, loc='upper left', frameon=True)\n",
        "plt.ylabel(\"Reshaped Correlation Coefficient\", fontsize=10)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.title(\"Regimes of Average Daily Correlation Coefficients\")\n",
        "plt.savefig('regimes_avg_correlation.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DyOGh0NDFkzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine Information on Regimes"
      ],
      "metadata": {
        "id": "JuNaKeUqFgQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "daily_avg_sentiment = daily_avg_sentiment.rename(columns={'regimes': 'regimes_sent'})\n",
        "daily_avg_holdings = daily_avg_holdings.rename(columns={'regimes': 'regimes_hold'})\n",
        "daily_avg_correlation = daily_avg_correlation.rename(columns={'regimes': 'regimes_corr'})\n",
        "\n",
        "total_regime_data = daily_avg_sentiment.merge(daily_avg_holdings, on='date')\n",
        "total_regime_data = total_regime_data.merge(daily_avg_correlation, on='date')\n",
        "\n",
        "# Box plots\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='regimes_sent', y='filtered_scores', data=total_regime_data)\n",
        "plt.xlabel('Regime')\n",
        "plt.ylabel('Sentiment Scores')\n",
        "plt.title('Box Plot of Sentiment Scores by Regime')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='regimes_hold', y='pct_change', data=total_regime_data)\n",
        "plt.xlabel('Regime')\n",
        "plt.ylabel('Changes in User Holdings')\n",
        "plt.title('Box Plot of Changes in User Holdings by Regime')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i392-6xFFgFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Look into different regimes a bit more"
      ],
      "metadata": {
        "id": "O66_HCLGFcSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comparison = pd.pivot_table(total_regime_data, values=['filtered_scores', 'pct_change'],\n",
        "                            index=['regimes_sent', 'regimes_hold'], aggfunc=np.mean)\n",
        "\n",
        "total_regime_data[total_regime_data['regimes_sent'] == 0].mean()\n",
        "total_regime_data[total_regime_data['regimes_sent'] == 1].mean()\n",
        "\n",
        "total_regime_data[total_regime_data['regimes_hold'] == 0].mean()\n",
        "total_regime_data[total_regime_data['regimes_hold'] == 1].mean()"
      ],
      "metadata": {
        "id": "cD-ZM5mgFb5c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}